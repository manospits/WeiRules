{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeiRules (Yahoo images classification)\n",
    "\n",
    "This is an example use of the neuro fuzzy model presented in:\n",
    "\n",
    "1. \"Pitsikalis, M., Do, TT., Lisitsa, A., Luo, S. (2021). Logic Rules Meet Deep Learning: A Novel Approach for Ship Type Classification. In: RuleML+RR 2021. https://doi.org/10.1007/978-3-030-91167-6_14\" and \n",
    "2. \"Logic Rules Meet Deep Learning: A Novel Approach for Ship Type Classification (Extended Abstract). Manolis Pitsikalis, Thanh-Toan Do, Alexei Lisitsa, Shan Luo. IJCAI Artificial Intelligence Sister Conferences Best Papers. Pages 5324-5328. https://doi.org/10.24963/ijcai.2022/744\"\n",
    "\n",
    "Since the images of the original maritime dataset are propriatery, we release the source code and an example application on a dataset of similar nature. \n",
    "\n",
    "Disclaimer: in what follows we will not use any hyperparameter tuning for any of the models, instead we will use the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import frcnn as fw\n",
    "import weirules as wr\n",
    "import w_utils as wutils\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import locale\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading & preparation\n",
    "\n",
    "For this experiment we are using the Yahoo test dataset that can be found here https://vision.cs.uiuc.edu/attributes/. The dataset contains images accompanied with bounding boxes and tabular data.\n",
    "\n",
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_names=[]\n",
    "with open('sample/yahoo/attribute_data/attribute_names.txt','r') as inp:\n",
    "    for line in inp:\n",
    "        attribute_names.append(line.rstrip())\n",
    "attribute_names_characteristics=attribute_names[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_columns=['xmin','ymin','xmax','ymax']\n",
    "columns= ['img','class']+box_columns+attribute_names\n",
    "yahoo_images=pd.read_csv('sample/yahoo/attribute_data/ayahoo_test.txt',delimiter=' ',header=None,names=columns)\n",
    "yahoo_images=yahoo_images.drop_duplicates(subset=['img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>2D Boxy</th>\n",
       "      <th>3D Boxy</th>\n",
       "      <th>Round</th>\n",
       "      <th>Vert Cyl</th>\n",
       "      <th>...</th>\n",
       "      <th>Wood</th>\n",
       "      <th>Cloth</th>\n",
       "      <th>Furry</th>\n",
       "      <th>Glass</th>\n",
       "      <th>Feather</th>\n",
       "      <th>Wool</th>\n",
       "      <th>Clear</th>\n",
       "      <th>Shiny</th>\n",
       "      <th>Vegetation</th>\n",
       "      <th>Leather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donkey_1.jpg</td>\n",
       "      <td>donkey</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>54</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>donkey_100.jpg</td>\n",
       "      <td>donkey</td>\n",
       "      <td>14</td>\n",
       "      <td>98</td>\n",
       "      <td>125</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>donkey_102.jpg</td>\n",
       "      <td>donkey</td>\n",
       "      <td>332</td>\n",
       "      <td>29</td>\n",
       "      <td>614</td>\n",
       "      <td>438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>donkey_111.jpg</td>\n",
       "      <td>donkey</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>295</td>\n",
       "      <td>244</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>donkey_113.jpg</td>\n",
       "      <td>donkey</td>\n",
       "      <td>32</td>\n",
       "      <td>158</td>\n",
       "      <td>159</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>carriage_921.jpg</td>\n",
       "      <td>carriage</td>\n",
       "      <td>147</td>\n",
       "      <td>111</td>\n",
       "      <td>314</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>carriage_922.jpg</td>\n",
       "      <td>carriage</td>\n",
       "      <td>70</td>\n",
       "      <td>82</td>\n",
       "      <td>229</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>carriage_923.jpg</td>\n",
       "      <td>carriage</td>\n",
       "      <td>5</td>\n",
       "      <td>173</td>\n",
       "      <td>256</td>\n",
       "      <td>348</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642</th>\n",
       "      <td>carriage_937.jpg</td>\n",
       "      <td>carriage</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>189</td>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>carriage_99.jpg</td>\n",
       "      <td>carriage</td>\n",
       "      <td>37</td>\n",
       "      <td>173</td>\n",
       "      <td>405</td>\n",
       "      <td>514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2237 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   img     class  xmin  ymin  xmax  ymax  2D Boxy  3D Boxy  \\\n",
       "0         donkey_1.jpg    donkey    14    21    54    72        0        0   \n",
       "2       donkey_100.jpg    donkey    14    98   125   214        0        0   \n",
       "3       donkey_102.jpg    donkey   332    29   614   438        0        0   \n",
       "4       donkey_111.jpg    donkey    23    52   295   244        0        0   \n",
       "5       donkey_113.jpg    donkey    32   158   159   286        0        0   \n",
       "...                ...       ...   ...   ...   ...   ...      ...      ...   \n",
       "2639  carriage_921.jpg  carriage   147   111   314   184        0        1   \n",
       "2640  carriage_922.jpg  carriage    70    82   229   205        0        0   \n",
       "2641  carriage_923.jpg  carriage     5   173   256   348        0        0   \n",
       "2642  carriage_937.jpg  carriage     9     4   189   147        0        0   \n",
       "2643   carriage_99.jpg  carriage    37   173   405   514        0        0   \n",
       "\n",
       "      Round  Vert Cyl  ...  Wood  Cloth  Furry  Glass  Feather  Wool  Clear  \\\n",
       "0         0         0  ...     0      0      1      0        0     0      0   \n",
       "2         0         0  ...     0      0      0      0        0     0      0   \n",
       "3         0         0  ...     0      0      1      0        0     0      0   \n",
       "4         0         0  ...     0      0      1      0        0     0      0   \n",
       "5         0         0  ...     0      0      1      0        0     0      0   \n",
       "...     ...       ...  ...   ...    ...    ...    ...      ...   ...    ...   \n",
       "2639      0         0  ...     1      0      0      0        0     0      0   \n",
       "2640      0         0  ...     0      0      0      0        0     0      0   \n",
       "2641      0         0  ...     1      0      0      0        0     0      0   \n",
       "2642      0         0  ...     0      0      0      0        0     0      0   \n",
       "2643      0         0  ...     0      0      0      0        0     0      0   \n",
       "\n",
       "      Shiny  Vegetation  Leather  \n",
       "0         0           0        0  \n",
       "2         0           0        0  \n",
       "3         0           0        0  \n",
       "4         0           0        0  \n",
       "5         0           0        0  \n",
       "...     ...         ...      ...  \n",
       "2639      0           0        0  \n",
       "2640      0           0        0  \n",
       "2641      0           0        0  \n",
       "2642      0           0        0  \n",
       "2643      0           0        0  \n",
       "\n",
       "[2237 rows x 70 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yahoo_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two images contained invalid bounding boxes as the xmin was equal to xmax (see below). The two invalid bounding boxes were corrected manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yahoo_images[yahoo_images['xmin'] >= yahoo_images['xmax']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoder\n",
    "Use of label encoder to transform the n classes to the range [0,n-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "yahoo_images['class']=le.fit_transform(yahoo_images['class'])\n",
    "classes=yahoo_images['class'].unique()\n",
    "classes.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - testing set split\n",
    "The test size = 0.49 for the following experiment was set to 0.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y=yahoo_images['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(yahoo_images, y, test_size=0.49, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_train=x_train[attribute_names_characteristics]\n",
    "attributes_test=x_test[attribute_names_characteristics]\n",
    "\n",
    "class_labels_train=y_train\n",
    "class_labels_test=y_test\n",
    "\n",
    "image_paths_train=x_train['img']\n",
    "image_paths_test=x_test['img']\n",
    "\n",
    "boxes_train=x_train[box_columns]\n",
    "boxes_test=x_test[box_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The steps for training WeiRules are the following:\n",
    "  1. Fit a decision tree (or trees) on the tabular data, extract the rules, and create the network with the fuzzified rules.\n",
    "  2. Train Faster R-CNN on the images with the bounding boxes and the labels\n",
    "  3. Extract deep features corresponding to the highest score yielding box\n",
    "  4. Train WeiRules using the extracted deep features and the tabular data corresponding to the image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule extraction and fuzzification\n",
    "\n",
    "Create decision trees, extract the rules, and then create the neuro-fuzzy the network. `fit_tree` in this case, will learn a separate tree for each class by substituting the remaining classes with -1, therefore treating the problem locally as a binary classification task. The result here would be a set of rules each corresponding to each class. Setting `find_best_tree=true` will search for the tree depth up to `max_depth` that yields the highest macro F1-score on the training set. \n",
    "\n",
    "Single tree is also possible by setting `forest=False`. \n",
    "\n",
    "Uncomment the code below if you want to recreate the tree and the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wrmodel=wr.weirules(rule_learner='tree', use_weights=True)\n",
    "# wrmodel.fit_tree(X=attributes_train,\n",
    "#              Y=class_labels_train,\n",
    "#              en_classes=np.array(classes),\n",
    "#              rule_columns=attribute_names_characteristics,\n",
    "#              forest=True,\n",
    "#              max_depth=30)\n",
    "# wrmodel.create_network(rho=5.4)\n",
    "# # save the model\n",
    "# with open('./models/weirulesV6c-yahoo-r54-f.pkl', 'wb') as output:\n",
    "#      pickle.dump(wrmodel, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pickled version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/weirulesV6c-yahoo-r54-f.pkl', 'rb') as input:\n",
    "     wrmodel = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (dropout2d_1): Dropout2d(p=0.2, inplace=False)\n",
      "  (batchnorm2d1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2d2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (c_hidden_1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "  (batchnorm1d_c1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (batchnorm1d_4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (merge_hidden): Linear(in_features=512, out_features=274, bias=True)\n",
      "  (ors): ModuleList(\n",
      "    (0): minmax(in_features=5, out_features=1, bias=False)\n",
      "    (1): minmax(in_features=3, out_features=1, bias=False)\n",
      "    (2): minmax(in_features=3, out_features=1, bias=False)\n",
      "    (3): minmax(in_features=3, out_features=1, bias=False)\n",
      "    (4): minmax(in_features=9, out_features=1, bias=False)\n",
      "    (5): minmax(in_features=4, out_features=1, bias=False)\n",
      "    (6): minmax(in_features=3, out_features=1, bias=False)\n",
      "    (7): minmax(in_features=3, out_features=1, bias=False)\n",
      "    (8): minmax(in_features=5, out_features=1, bias=False)\n",
      "    (9): minmax(in_features=3, out_features=1, bias=False)\n",
      "    (10): minmax(in_features=5, out_features=1, bias=False)\n",
      "    (11): minmax(in_features=6, out_features=1, bias=False)\n",
      "  )\n",
      "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(wrmodel.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster R-CNN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training and testing dataset instances to use in data loaders for Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder='sample/yahoo/ayahoo_test_images/'\n",
    "\n",
    "#train\n",
    "index_train=pd.DataFrame(image_paths_train.index,index=image_paths_train.index,columns=['index'])\n",
    "dataset_train=pd.concat([index_train,image_paths_train,boxes_train,attributes_train], axis=1)\n",
    "dataset_train.to_csv('sample/yahoo/train_data.csv', sep=' ')\n",
    "yahoo_dataset_train_set = wutils.YahooDataset(dataset_train.values, class_labels_train.values, 6, image_folder)\n",
    "\n",
    "#test\n",
    "index_test=pd.DataFrame(image_paths_test.index,index=image_paths_test.index,columns=['index'])\n",
    "dataset_test=pd.concat([index_test,image_paths_test,boxes_test,attributes_test], axis=1)\n",
    "dataset_test.to_csv('sample/yahoo/test_data.csv', sep=' ')\n",
    "yahoo_dataset_test_set = wutils.YahooDataset(dataset_test.values, class_labels_test.values, 6, image_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "        'batch_size': 25, \n",
    "        'shuffle': True,\n",
    "        'num_workers': 0,\n",
    "        'collate_fn': collate_fn,\n",
    "        }\n",
    "\n",
    "max_epochs=100\n",
    "training_generator = data.DataLoader(yahoo_dataset_train_set, **params)\n",
    "testing_generator = data.DataLoader(yahoo_dataset_test_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Faster R-CNN with resnet50 backbone. (Uncomment if you want to retrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = fw.resnet_fpn_backbone('resnet50', pretrained=True)\n",
    "model = fw.FasterRCNN_Weirules(backbone, len(classes)+1)\n",
    "model = model.cuda()\n",
    "\n",
    "#frcnn_optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0005)\n",
    "#wutils.train_frcnn(model, training_generator, frcnn_optimizer, max_epochs)\n",
    "\n",
    "#torch.save(model.state_dict(), './models/frcnn_100_epochs_yahoo.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load saved model trained with the above parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./models/frcnn_100_epochs_yahoo.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep feature extraction\n",
    "\n",
    "Extract the corresponding deep features for the training and test set. (Uncomment if you want to re-extract them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#deep_features_dict_train=wutils.extract_df(model,training_generator)\n",
    "#with open('models/df_frcnn_100e_yahoo_train.pickle', 'wb') as output:\n",
    "#       pickle.dump(deep_features_dict_train, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#deep_features_dict_test=wutils.extract_df(model,testing_generator)\n",
    "#with open('models/df_frcnn_100e_yahoo_test.pickle', 'wb') as output:\n",
    "#      pickle.dump(deep_features_dict_test, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pickled extracted deep features dict. The keys of the dictionary use the index column of the dataset, while the values include the extracted deep feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_features_dict_train={}\n",
    "with open('models/df_frcnn_100e_yahoo_train.pickle', 'rb') as handle:\n",
    "    deep_features_dict_train=pickle.load(handle)\n",
    "deep_features_dict_test={}\n",
    "with open('models/df_frcnn_100e_yahoo_test.pickle', 'rb') as handle:\n",
    "    deep_features_dict_test=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WeiRules training\n",
    "\n",
    "Create dataset instances that include the deep feature dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_dataset_train_set_wr = wutils.YahooDataset(dataset_train.values, class_labels_train.values, 6, image_folder,deep_features_dict_train)\n",
    "yahoo_dataset_test_set_wr = wutils.YahooDataset(dataset_test.values, class_labels_test.values, 6, image_folder,deep_features_dict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the training parameters and the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params_wr = {\n",
    "        'batch_size': 64,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4,\n",
    "        'collate_fn': collate_fn,\n",
    "        }\n",
    "\n",
    "max_epochs=100\n",
    "training_generator_wr = data.DataLoader(yahoo_dataset_train_set_wr, **params_wr)\n",
    "test_generator_wr = data.DataLoader(yahoo_dataset_test_set_wr, **params_wr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the WeiRules network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_wrmodel_path='models/weirulesV6c-yahoo-r54-f.model'\n",
    "#weirules_optimizer = torch.optim.Adam(wrmodel.model.parameters(), lr=0.0001)\n",
    "#losses=wutils.train_weirules(wrmodel, training_generator_wr, weirules_optimizer, max_epochs)\n",
    "#torch.save(wrmodel.model.state_dict(), trained_wrmodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrmodel.load_model(trained_wrmodel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WeiRules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a fuzzified rule. Comparisons have been replaced with the appropriate form of the sigmoidal membership function, while conjunctions/disjunctions are approximated using the (weighted) exponential mean approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF\n",
      "\t0.27915024757385254 * (Wing <= 0.5 AND' Skin <= 0.5 AND' Hair > 0.5 AND' Furry <= 0.5)\n",
      "\t OR' 0.27915024757385254 * (Wing <= 0.5 AND' Skin > 0.5 AND' Nose <= 0.5 AND' Eye > 0.5)\n",
      "\t OR' 0.27915024757385254 * (Wing > 0.5)\n",
      "THEN 3\n"
     ]
    }
   ],
   "source": [
    "print(wrmodel.get_fuzzified_rule_str(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_best_results=wutils.predict_weirules(wrmodel, test_generator_wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlabel=[]\n",
    "alabel=[]\n",
    "for res in all_best_results:\n",
    "    wlabel.append(res[0])\n",
    "    alabel.append(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.78      0.85       137\n",
      "           1       0.90      0.80      0.85       104\n",
      "           2       0.83      0.56      0.67        72\n",
      "           3       1.00      0.67      0.80        24\n",
      "           4       0.72      0.33      0.46        63\n",
      "           5       0.90      0.77      0.83        73\n",
      "           6       0.98      0.97      0.98       179\n",
      "           7       0.93      0.97      0.95        76\n",
      "           8       0.58      0.99      0.73       103\n",
      "           9       0.98      0.90      0.94        94\n",
      "          10       0.50      0.70      0.58        84\n",
      "          11       0.64      0.69      0.66        88\n",
      "\n",
      "    accuracy                           0.80      1097\n",
      "   macro avg       0.82      0.76      0.77      1097\n",
      "weighted avg       0.83      0.80      0.80      1097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(alabel,wlabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster R-CNN\n",
    "\n",
    "It would be more sensible to use an object detection metric such mAP for Faster R-CNN, since each image has one class we can assume take the prediction with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {\n",
    "        'batch_size': 1, # change to higher number for faster results if enough memory is available\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0,\n",
    "        'collate_fn': collate_fn,\n",
    "        }\n",
    "\n",
    "testing_generator = data.DataLoader(yahoo_dataset_test_set, **params)\n",
    "all_best_results=wutils.predict_frcnn(model, testing_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       136\n",
      "           1       0.85      0.91      0.88       102\n",
      "           2       0.78      0.86      0.82        72\n",
      "           3       0.29      0.42      0.34        24\n",
      "           4       0.51      0.71      0.59        63\n",
      "           5       0.66      0.53      0.59        73\n",
      "           6       0.97      0.94      0.95       179\n",
      "           7       0.66      0.64      0.65        76\n",
      "           8       0.85      0.86      0.86       103\n",
      "           9       0.80      0.59      0.68        93\n",
      "          10       0.74      0.67      0.70        83\n",
      "          11       0.96      0.91      0.94        88\n",
      "\n",
      "    accuracy                           0.79      1092\n",
      "   macro avg       0.74      0.74      0.73      1092\n",
      "weighted avg       0.80      0.79      0.79      1092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flabel = []\n",
    "alabel = []\n",
    "for res in all_best_results[0]:\n",
    "    flabel.append(res[1]['label']-1)\n",
    "    alabel.append(res[2])\n",
    "\n",
    "print(classification_report(alabel, flabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "Decision tree with no hyper parameter optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.98      0.71       137\n",
      "           1       0.89      0.65      0.76       104\n",
      "           2       1.00      0.31      0.47        72\n",
      "           3       0.95      0.75      0.84        24\n",
      "           4       0.69      0.32      0.43        63\n",
      "           5       0.92      0.78      0.84        73\n",
      "           6       1.00      0.96      0.98       179\n",
      "           7       0.96      1.00      0.98        76\n",
      "           8       0.94      0.91      0.93       103\n",
      "           9       0.97      0.90      0.93        94\n",
      "          10       0.43      0.67      0.52        84\n",
      "          11       0.48      0.44      0.46        88\n",
      "\n",
      "    accuracy                           0.77      1097\n",
      "   macro avg       0.82      0.72      0.74      1097\n",
      "weighted avg       0.81      0.77      0.76      1097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "clf = clf.fit(attributes_train, class_labels_train)\n",
    "preds=clf.predict(attributes_test)\n",
    "print(classification_report(class_labels_test, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weirules",
   "language": "python",
   "name": "weirules"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
